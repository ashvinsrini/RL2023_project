{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332ba632",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 7 - Model Based Reinforcement Learning </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Cross Entropy Method (CEM) </a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "* <a href='#4.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing CEM (30 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Changing Number of Samples (10 points)</a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Model-free vs Mode-based RL (20 points)</a>\n",
    "    \n",
    "**Total Points:** 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013318a6",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this exercise we will dive into model-based reinforcement learning. We will implement planning over several time steps. We will use the cross entropy method (CEM) to choose actions at each time step. CEM is often used in model-based reinforcement learning for choosing actions. The main working principles of CEM were explained during the lecture \"Model-based RL\". We use the simulator to simulate state transitions (another possibility would be to learn a dynamics model $s_{t+1} = f(s_t, a_t)$ to simulate state transitions if access to the system dynamics is not available).\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- Understand how to get CEM planning working in practice\n",
    "- Understand limitations and advantages of model-based RL using CEM\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "You don‚Äôt have to edit any other file other than ```ex7.ipynb``` to complete this exercise.\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                 # Images used in notebook\n",
    "‚îÇ   ex7_CEM.ipynb        # Main assignment file containing tasks <---------\n",
    "‚îÇ   env.py               # Wrappers for the environment\n",
    "|   simulator.py         # Using the exact environment as the model (simulator)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197a537",
   "metadata": {},
   "source": [
    "# 2. Cross Entropy Method (CEM) <a id='2.'></a>\n",
    "\n",
    "In this section, we will try to solve the **Cup-Catch** environment from the [DeepMind Control Suite](https://github.com/deepmind/dm_control/tree/main/dm_control/suite) by planning using CEM. \n",
    "\n",
    "In **Cup-Catch**, a ball is attached to a string which hangs from a cup. The goal is to swing the ball into the cup by moving the cup vertically up and down. The task has a sparse reward: 1 when the ball is in the cup, 0 otherwise. In order to save computation time we select an action every six time steps and use a wrapper to repeat the same action 6 times. Therefore, the maximum reward for each actual time step is 6.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/cup_catcher.png\" width=\"400px\">\n",
    "    <figcaption> Figure 1: Cup-Catch environment </figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72ec92",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing CEM (30 points) </h3> \n",
    "\n",
    "You need to complete the planning part in the code marked as ```TODO```. The code takes advantage of multiple processor cores by parallelizing the code. For more information about parallelizing, please check [Joblib](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html). <br>\n",
    "    \n",
    "**Ensure that the notebook contains the reward plot.** \n",
    "\n",
    "The reference training plot is as Figure 2:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/cem_reward.png\">\n",
    "    <figcaption> Figure 2: Reward function at each time step in CEM </figcaption>\n",
    "</div>\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb546a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path # to find directory\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from typing import Sequence, Tuple, Dict, Callable, List\n",
    "from functools import partial\n",
    "import copy, torch, time\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "from env import make_env\n",
    "from simulator import SimulatorWrapper\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60671443-30df-4a7c-bffc-2e88a1fb70c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mediapy # install a package required for video visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47668253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CEM(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        action_shape,\n",
    "        num_samples,\n",
    "        num_topk,\n",
    "        plan_horizon,\n",
    "        iteration,\n",
    "        keep_last_solution,\n",
    "        expl_noise\n",
    "    ):\n",
    "\n",
    "        self.model = model # the dynamics model\n",
    "\n",
    "        self.action_dim = action_shape[0]\n",
    "        self.num_samples = num_samples\n",
    "        self.num_topk = num_topk\n",
    "        self.plan_horizon = plan_horizon\n",
    "        self.iteration = iteration\n",
    "        self.keep_last_solution = keep_last_solution\n",
    "        self.expl_noise = expl_noise\n",
    "\n",
    "        # init simulator\n",
    "        o = self.model.reset()\n",
    "        self.model.save_checkpoint()\n",
    "        \n",
    "    def plan(self, obs, t0, eval_mode=False):\n",
    "        if obs.ndim == 1: obs = obs[None] # add batch dim\n",
    "        # initialize paramters\n",
    "        mean = np.zeros((self.plan_horizon, self.action_dim))\n",
    "        std = np.ones_like(mean)\n",
    "        # use previous plan as start point if not at the first step\n",
    "        if not t0 and hasattr(self, \"_prev_mean\"):\n",
    "            mean[:-1] = copy.copy(self._prev_mean[1:])\n",
    "\n",
    "        with Parallel(n_jobs=-1,) as parallel:  # we use joblib.Parallel to parallel the evaluation.\n",
    "            # Iterate CEM\n",
    "            for _ in range(self.iteration):\n",
    "                # TODO: Implement Cross-Entropy Method\n",
    "                \n",
    "                # Hints: \n",
    "                # 1. Generate random actions using Gaussian distribution with mean and std as parameters. \n",
    "                #    Use self.num_samples as the number of samples. Clip the samples to (-1, 1).\n",
    "                # 2. Perform Monte Carlo evaluation by computing the episode return for each sample using self.model as follows:\n",
    "                #    2.1. Use parallel(delayed(rollout_simulator)(self.model, action_sample) for each sample from 1.\n",
    "                # 3. Select top self.num_topk actions (elite actions) using episode returns from 2.1. Use numpy.argpartition.\n",
    "                # 4. Compute mean and std of elite actions and assign it to mean and std used in 1.\n",
    "                \n",
    "                ########## Your code starts here. ##########\n",
    "                \n",
    "                ########## Your code ends here. ##########\n",
    "\n",
    "        if self.keep_last_solution:\n",
    "            self._prev_mean = mean\n",
    "\n",
    "        # select the first action in the planed horizon\n",
    "        action, std = mean[0], std[0]\n",
    "\n",
    "        if not eval_mode:\n",
    "            action += self.expl_noise * np.random.randn(action.shape)\n",
    "\n",
    "        # update the simulator state since simulator is used to do planning\n",
    "        next_obs, reward, done, info = self.model.step(action)\n",
    "        self.model.save_checkpoint()\n",
    "        \n",
    "        return action, info  \n",
    "        \n",
    "\n",
    "def rollout_simulator(model, traj):\n",
    "    model.load_checkpoint()\n",
    "\n",
    "    terminated, episode_return = False, 0\n",
    "    for act in traj:\n",
    "        obs, reward, done, _ = model.step(act)\n",
    "        reward = 0 if terminated else reward\n",
    "\n",
    "        terminated |= bool(done)\n",
    "        episode_return += reward\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "        \n",
    "    return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0dfc4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note because of the action repeat a reward of 6 can be achieved from multiple steps being taken where the ball in the cup\n",
    "eval_env = make_env(\n",
    "    env_name='cup-catch',\n",
    "    seed=1,\n",
    "    action_repeat=6,\n",
    "    modality='pixels', \n",
    "    frame_stack=1,\n",
    "    img_size=(240, 320)\n",
    ")\n",
    "\n",
    "model_env = make_env(\n",
    "    env_name='cup-catch',\n",
    "    seed=1,\n",
    "    action_repeat=6\n",
    ")\n",
    "\n",
    "obs_shape = tuple(int(x) for x in eval_env.observation_space.shape)\n",
    "action_shape = tuple(int(x)  for x in eval_env.action_space.shape)\n",
    "\n",
    "model = SimulatorWrapper(model_env)\n",
    "\n",
    "agent = CEM(\n",
    "    model=model,\n",
    "    action_shape = action_shape,\n",
    "    num_samples=50,\n",
    "    num_topk=5,\n",
    "    plan_horizon=12,\n",
    "    iteration=5,\n",
    "    keep_last_solution=True,\n",
    "    expl_noise=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5bdc7-3a44-4c6e-9f27-d9550b9cc0a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs, done, ep_reward, t = eval_env.reset(), False, 0, 0\n",
    "rewards, observations = [], []\n",
    "while not done and t < 20:\n",
    "    action, info = agent.plan(obs, eval_mode=True, t0=(t==0))\n",
    "    obs, reward, done, _ = eval_env.step(action)\n",
    "    rewards.append(reward)\n",
    "    observations.append(obs)\n",
    "    \n",
    "    print(f'Timestep: {t} Reward: {reward}')\n",
    "    ep_reward += reward\n",
    "\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4399036-8ac8-450e-8abc-924b7cdb0d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mediapy \n",
    "mediapy.write_video('video.mp4', [observations[i].transpose(1, 2, 0) for i in range(len(observations))], fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4f167-2f92-4239-aca4-da38c0923882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video # to display videos\n",
    "Video(Path().cwd()/'video.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabdf29-028e-41af-b8fc-57e5eee59c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(rewards, linewidth=1.2, color='b')\n",
    "plt.xticks(list(range(0, len(rewards), 2)))\n",
    "plt.xlabel('Time step', fontsize=10)\n",
    "plt.ylabel('Reward', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76b6fa",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1</b> Changing Number of Samples (5 points)</h3> \n",
    "\n",
    "Discuss the effect of changing the __number of samples__. How can this affect the performance and running time?            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496c2df",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611beaf",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Model-free vs Mode-based RL (20 points)</h3> \n",
    "\n",
    "Assume that the dynamics model $s_{t+1} = f(s_t, a_t)$ (when using a probabilistic dynamics model $P(s_{t+1}| s_t, a_t)$) and reward model $R(s_t, a_t)$ are learned from data during training. CEM with learned models can be used to solve similar tasks as model-free reinforcement learning methods such as DDPG.\n",
    "<br>\n",
    "In what kind of tasks do you expect CEM with learned models to work better than DDPG? In which kind of tasks do you expect DDPG to work better than CEM with learned models in terms of performance and training time? Why?\n",
    "<br>\n",
    "Which parts of CEM with a learned dynamics model need to be taken into account when considering computation time and why?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d8a7e-4bb1-463e-88ab-62270e20d05e",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e950c66-7183-4d2e-afad-76d44e87ea7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex7_MCTS.ipynb```) are answered and the relevant plots are recorded in the relevant places. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e25649-58a0-448c-9fbc-7e055b20bc84",
   "metadata": {},
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb0232-db2a-4e14-b075-9ef187b43b4b",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac81f90-8902-4ed9-922b-b8ded5327c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52473cec-5ab3-439f-97ee-85c8796ec469",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b2268-cbe3-4df8-a6d0-665aa425f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = None # Student Task 1. Implementing CEM\n",
    "Q1 = None # Question 1.1: Number of samples\n",
    "Q2 = None # Question 2.1: Model-free vs Model-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3e791-b9fc-479c-871d-df4c85ab7dce",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8c9dd-26cb-4188-90e1-b7fad675da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = None # Student Task 1. Implementing CEM\n",
    "Q1 = None # Question 1.1: Number of samples\n",
    "Q2 = None # Question 2.1: Model-free vs Model-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11c47c-3716-4642-adc5-c4569ac5fd8f",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fd0e4-b0fe-4b59-8c8a-c0a51c24d5fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Please use the following section to record references.\n",
    "# References <a id='4.'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552605a-ecef-401e-ae00-7a7009725bd4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb61e66",
   "metadata": {},
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could\n",
    "answer feedback questionnaire in ```feedback.ipynb```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
